# -*- coding: utf-8 -*-
"""RNN_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10OuSw_gHXKyJPmQ1zju2-eBxCJOwYTJd
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pickle
import seaborn as sns

from sklearn.model_selection import train_test_split

import tensorflow
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import Sequential, Model
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import optimizers
from tensorflow.keras.preprocessing.text import Tokenizer

import json
import io
from google.colab import files

import nltk
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer

tensorflow.executing_eagerly()

uploaded = files.upload()
file_name = "data.json"
io.StringIO(uploaded["data.json"].decode("utf-8"))
data = json.loads(uploaded[file_name].decode("utf-8"))

df = pd.DataFrame(data, columns = ["text"])
df

vds = SentimentIntensityAnalyzer()

df["label"] = df["text"].apply(lambda x: (vds.polarity_scores(x)))
df["label"] = df["label"].apply(lambda x: np.array(1) if x["compound"] > 0.1 is 1 else np.array(2) if x["compound"] < -0.1  else np.array(0))
df

y = df["label"]
x = df["text"]
x_train, x_val, y_train, y_val = train_test_split(x, y, test_size = 0.2, random_state = 123)

num_words = 50000
output_dims = 200
tokenizer = Tokenizer(oov_token = "<OOV>", num_words = num_words)
tokenizer.fit_on_texts(x_train)

train_seqs = tokenizer.texts_to_sequences(x_train)
val_seqs = tokenizer.texts_to_sequences(x_val)

input_dims = len(tokenizer.word_index) + 1

# maxlen = max([len(x) for x in train_seqs])
maxlen = 200
train_seqs = pad_sequences(train_seqs, padding = "post", maxlen = maxlen)
val_seqs = pad_sequences(val_seqs, padding = "post", maxlen = maxlen)

print("train seqs shape: ", train_seqs.shape)
print('input dimensions: ', input_dims)
print('output dimensions: ', output_dims)

train_seqs = tensorflow.convert_to_tensor(train_seqs)
val_seqs = tensorflow.convert_to_tensor(val_seqs)

tensorflow.keras.backend.clear_session()
model = Sequential([
    layers.Embedding(input_dim = input_dims, output_dim = 20, mask_zero = True),
    layers.GRU(20, recurrent_dropout = 0.3),
    layers.Dense(10, activation = "relu"),
    layers.Dropout(0.5),
    layers.Dense(12466, activation = "softmax")
])

model.compile(optimizer = optimizers.Adam(learning_rate = 0.0005), 
              loss = "sparse_categorical_crossentropy", 
              metrics = ["accuracy"])

model.summary()

gru_history = model.fit(train_seqs, y_train,
                        validation_data = (val_seqs, y_val),
                        batch_size = 256,
                        epochs = 50,
                        callbacks = EarlyStopping(monitor='val_loss', patience=5))

train_seqs.shape, y_train.shape, val_seqs.shape, y_val.shape

model.predict("Thank you so much I will try my best:) hope u have an amazing day")

